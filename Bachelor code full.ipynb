{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A short project introduction\n",
    "Machine learning algorithms on a dataset about bank term deposits\n",
    "\n",
    "This is code from my bachelor's thesis, where we tested four different machine learning and prediction models to find the optimal one for predicting customers willing to sign up for a term deposit. This code has been cleaned and restructured later to better fit GitHub, where I mostly post the programming part of major assignments and not the assignment as a whole.\n",
    "\n",
    "It should be prefaced that this was a group assignment. Although I have gone through and rewritten a lot of the original code, it was a collaborative work. Therefore, I would like to thank Henrik Krantz Knudsen, Jakob Lindstrøm, and Joakim Sælemyr for their contributions to this project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1 Data management, cleaning and resampling\n",
    "\n",
    "<h2>1.1 Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import seaborn as sns\n",
    "from seaborn import heatmap as hm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score as cvs\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier as dtc\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45211, 43)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing dataset\n",
    "df=pd.read_csv(\"BankData.csv\",sep=\";\")\n",
    "df.shape\n",
    "\n",
    "#Converting the binary value objects into int using the LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "#Convert all strings into numbers\n",
    "df[\"Def_Credit\"]=le.fit_transform(df.default)\n",
    "df[\"House_loan\"]=le.fit_transform(df.housing)\n",
    "df[\"Pers_loan\"]=le.fit_transform(df.loan)\n",
    "df[\"Deposit\"]=le.fit_transform(df.y)\n",
    "\n",
    "#Looking at the new dataframe\n",
    "list=[\"default\",\"housing\",\"loan\",\"y\"]\n",
    "df=df.drop(columns=list)\n",
    "df=pd.get_dummies(df, drop_first=True)\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1.2 Splitting and resampling\n",
    "\n",
    "<h3> 1.2.1 Original train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into original train set and universal test size\n",
    "orgtrain, test = train_test_split(df, test_size=.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.2.2 Undersample train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retriving all 1s from original train set\n",
    "ones = orgtrain.loc[orgtrain.Deposit == 1]\n",
    "\n",
    "#Retriving random zeros from the original train set\n",
    "alfa, randomzeros = train_test_split(orgtrain.loc[orgtrain.Deposit==0], test_size=.13255, random_state=42)\n",
    "\n",
    "# Creating the undersampled training dataset\n",
    "under = pd.concat([ones, randomzeros], axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.2.3 Oversample set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving all 0s from original train set\n",
    "allzeros = orgtrain.loc[orgtrain.Deposit == 0]\n",
    "\n",
    "#Replicating all the ones from original train set seven times,\n",
    "#and concats it into one df\n",
    "newone = ones\n",
    "for i in range(0,6,1):\n",
    "    newone = pd.concat([newone,ones],axis=0)\n",
    "\n",
    "#Retrieving random 1s from original train set \n",
    "#so the amount of 1s and 0s is matching.\n",
    "charlie, additionalones = train_test_split(ones, test_size=0.54429, random_state=42)\n",
    "\n",
    "#adding all the ones to create one large df with only 1s.\n",
    "totalones = pd.concat([newone,additionalones], axis=0)\n",
    "\n",
    "#Concating the 0s and 1s into one oversample df.\n",
    "over = pd.concat([totalones, allzeros],axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.2.4 Creating csv-files out of our train- and test sets and reloading them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating individual csv-files for each dataset. This made it easier to distribute the same data to the whole group.\n",
    "under.to_csv('under.csv')\n",
    "test.to_csv('test.csv')\n",
    "over.to_csv('over.csv')\n",
    "orgtrain.to_csv('org.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading csv-files\n",
    "test = pd.read_csv('test.csv')\n",
    "orgtrain = pd.read_csv('org.csv')\n",
    "over  = pd.read_csv('over.csv')\n",
    "under = pd.read_csv('under.csv')\n",
    "del test['Unnamed: 0']\n",
    "del orgtrain['Unnamed: 0']\n",
    "del over['Unnamed: 0']\n",
    "del under['Unnamed: 0']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1.2.5 Defining Variables and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining columns in a list\n",
    "columns = ['age', 'balance', 'day',\n",
    " 'duration', 'campaign', 'pdays','previous', 'Def_Credit', 'House_loan',\n",
    " 'Pers_loan', 'job_blue-collar', 'job_entrepreneur',\n",
    " 'job_housemaid', 'job_management',\n",
    " 'job_retired', 'job_self-employed', 'job_services',\n",
    " 'job_student', 'job_technician', 'job_unemployed', 'job_unknown',\n",
    " 'marital_married', 'marital_single', 'education_secondary',\n",
    " 'education_tertiary', 'education_unknown', 'contact_telephone',\n",
    " 'contact_unknown','month_aug', 'month_dec','month_feb',\n",
    " 'month_jan','month_jul','month_jun', 'month_mar',\n",
    " 'month_may','month_nov','month_oct',\n",
    " 'month_sep','poutcome_other','poutcome_success','poutcome_unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining test data\n",
    "df_test = test\n",
    "\n",
    "# Defining training data\n",
    "df_org_train = orgtrain\n",
    "df_under_train = under\n",
    "df_over_train = over \n",
    "\n",
    "# Defining targets and varaibles for all datasets\n",
    "org_train_target = df_org_train[\"Deposit\"]\n",
    "org_train_data = df_org_train[columns]\n",
    "\n",
    "under_train_target = df_under_train[\"Deposit\"]\n",
    "under_train_data = df_under_train[columns]\n",
    "\n",
    "over_train_target = df_over_train[\"Deposit\"]\n",
    "over_train_data = df_over_train[columns]\n",
    "\n",
    "test_target = df_test[['Deposit']]\n",
    "test_data = df_test[columns]\n",
    "\n",
    "# Defining lists of datasets to use in functions\n",
    "data = [org_train_data, under_train_data, over_train_data]\n",
    "targets = [org_train_target, under_train_target, over_train_target]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a function looping through all logistic regression scores\n",
    "\n",
    "def Logistic_Regression():\n",
    "    model = LogisticRegression(solver='lbfgs',max_iter=95000)\n",
    "\n",
    "    train_res = []\n",
    "    test_res = []\n",
    "    pcl_res = []\n",
    "    \n",
    "    for i in range(0,3):\n",
    "        model = model.fit(data[i], targets[i])\n",
    "\n",
    "        # Attaining scores and saving them\n",
    "        score_train = model.score(data[i], targets[i])\n",
    "        train_res.append(score_train)\n",
    "\n",
    "        score_test = model.score(test_data, test_target)\n",
    "        test_res.append(score_test)\n",
    "\n",
    "        cm = confusion_matrix(model.predict(test_data), test_target)\n",
    "        cm = cm/np.sum(cm)\n",
    "        cm = cm[0][1]/(cm[0][1]+cm[0][0])\n",
    "        pcl_res.append(cm)\n",
    "\n",
    "    df = pd.DataFrame({\"Train\": train_res, \"Test\": test_res, \"PCL\": pcl_res}, index=[\"Un-sampled\", \"Undersampled\", \"Oversampled\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>PCL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Un-sampled</th>\n",
       "      <td>0.903063</td>\n",
       "      <td>0.898927</td>\n",
       "      <td>0.083747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Undersampled</th>\n",
       "      <td>0.835586</td>\n",
       "      <td>0.839876</td>\n",
       "      <td>0.027737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oversampled</th>\n",
       "      <td>0.832404</td>\n",
       "      <td>0.841756</td>\n",
       "      <td>0.027258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Train      Test       PCL\n",
       "Un-sampled    0.903063  0.898927  0.083747\n",
       "Undersampled  0.835586  0.839876  0.027737\n",
       "Oversampled   0.832404  0.841756  0.027258"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogReg_results = Logistic_Regression()\n",
    "LogReg_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 3 Decision Tree Classsifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decision_Tree_Classifier():\n",
    "    model = dtc()\n",
    "\n",
    "    train_res = []\n",
    "    test_res = []\n",
    "    pcl_res = []\n",
    "\n",
    "    ideal_params = []\n",
    "\n",
    "    for i in range(0,3):\n",
    "        # Using pruning to itterate through alpha values\n",
    "        model.fit(data[i], targets[i])\n",
    "        path = model.cost_complexity_pruning_path(data[i], targets[i])\n",
    "        ccp_alphas = path.ccp_alphas\n",
    "        ccp_alphas = ccp_alphas[:-1]\n",
    "        \n",
    "        # Finding the optimal alpha value through itteration\n",
    "        alpha_loop_values = []\n",
    "        for alpha in ccp_alphas:\n",
    "            model = dtc(ccp_alpha = alpha, random_state=42)\n",
    "            scores = cvs(model, data[i], targets[i], cv=5)\n",
    "            alpha_loop_values.append([alpha, np.mean(scores), np.std(scores)])\n",
    "        \n",
    "        alpha_res = pd.DataFrame(alpha_loop_values, columns=[\"Alpha\", \"Mean_Accuracy\", \"STD_Accuracy\"])\n",
    "        ideal_alpha = alpha_res[alpha_res.Mean_Accuracy == max(alpha_res.Mean_Accuracy)].Alpha\n",
    "\n",
    "        # Incase there are several optimal alpha values\n",
    "        if len(ideal_alpha) > 1:\n",
    "            ideal_alpha = ideal_alpha.iloc[-1]\n",
    "        ideal_alpha = float(ideal_alpha)\n",
    "\n",
    "        # Fitting the ideal model\n",
    "        model = dtc(ccp_alpha = ideal_alpha)\n",
    "        model = model.fit(data[i], targets[i])\n",
    "\n",
    "        # Attaining scores and saving them\n",
    "        train_score = model.score(data[i], targets[i])\n",
    "        train_res.append(train_score)\n",
    "\n",
    "        test_score = model.score(test_data, test_target)\n",
    "        test_res.append(test_score)\n",
    "\n",
    "        cm = confusion_matrix(model.predict(test_data), test_target)\n",
    "        cm = cm/np.sum(cm)\n",
    "        cm = cm[0][1]/(cm[0][1]+cm[0][0])\n",
    "        pcl_res.append(cm)\n",
    "\n",
    "        ideal_params.append(ideal_alpha)\n",
    "\n",
    "    df = pd.DataFrame({\"Train\": train_res, \"Test\": test_res, \"PCL\": pcl_res}, index=[\"Un-sampled\", \"Undersampled\", \"Oversampled\"])\n",
    "\n",
    "    return df, ideal_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal alpah parameters [0.00027570515406692236, 0.00048409972122178107, 1.2450220732038378e-05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>PCL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Un-sampled</th>\n",
       "      <td>0.905607</td>\n",
       "      <td>0.898706</td>\n",
       "      <td>0.072124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Undersampled</th>\n",
       "      <td>0.870673</td>\n",
       "      <td>0.809134</td>\n",
       "      <td>0.020782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oversampled</th>\n",
       "      <td>0.999890</td>\n",
       "      <td>0.874820</td>\n",
       "      <td>0.077552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Train      Test       PCL\n",
       "Un-sampled    0.905607  0.898706  0.072124\n",
       "Undersampled  0.870673  0.809134  0.020782\n",
       "Oversampled   0.999890  0.874820  0.077552"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTC_function = Decision_Tree_Classifier()\n",
    "print(\"Ideal alpah parameters\", DTC_function[1])\n",
    "DTC_results = DTC_function[0]\n",
    "DTC_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 4 Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling variables\n",
    "org_data_scaled = scale(org_train_data)\n",
    "under_data_scaled = scale(under_train_data)\n",
    "over_data_scaled = scale(over_train_data)\n",
    "test_data_scaled = scale(test_data)\n",
    "\n",
    "data_svm = [org_data_scaled, under_data_scaled, over_data_scaled]\n",
    "\n",
    "# Using gridserch and low number of variables to avoid over-processing\n",
    "grid_params = [{ \"C\":[1, 5, 10], \"gamma\":[\"scale\",0.1, 0.01, 0.001], \"kernel\":[\"rbf\"] }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Support_Vector_Machine():\n",
    "    model = SVC(random_state=42)\n",
    "\n",
    "    train_res = []\n",
    "    test_res = []\n",
    "    pcl_res = []\n",
    "\n",
    "    ideal_params = []\n",
    "\n",
    "    for i in range(0,3):\n",
    "        # Finding optimal gamma and C parameters\n",
    "        optimal_params = GridSearchCV(SVC(), grid_params, refit=True, cv=3, scoring=\"accuracy\", verbose=0)\n",
    "        optimal_params = optimal_params.fit(data_svm[i], targets[i])\n",
    "\n",
    "        optimal_C = optimal_params.best_params_[\"C\"]\n",
    "        optimal_gamma = optimal_params.best_params_[\"gamma\"]\n",
    "\n",
    "        # Fitting the ideal model\n",
    "        model =SVC(random_state=42, C=optimal_C, gamma=optimal_gamma)\n",
    "        model.fit(data_svm[i], targets[i])\n",
    "\n",
    "        # Attaining scores and saving them\n",
    "        train_score = model.score(data_svm[i], targets[i])\n",
    "        train_res.append(train_score)\n",
    "\n",
    "        test_score = model.score(test_data_scaled, test_target)\n",
    "        test_res.append(test_score)\n",
    "\n",
    "        cm = confusion_matrix(model.predict(test_data_scaled), test_target)\n",
    "        cm = cm/np.sum(cm)\n",
    "        cm = cm[0][1]/(cm[0][1]+cm[0][0])\n",
    "        pcl_res.append(cm)\n",
    "\n",
    "    ideal_params.append([optimal_C, optimal_gamma])\n",
    "\n",
    "    df = pd.DataFrame({\"Train\": train_res, \"Test\": test_res, \"PCL\": pcl_res}, index=[\"Un-sampled\", \"Undersampled\", \"Oversampled\"])\n",
    "\n",
    "    return df, ideal_params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal C and gamma parameters [[10, 0.1]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>PCL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Un-sampled</th>\n",
       "      <td>0.921063</td>\n",
       "      <td>0.902687</td>\n",
       "      <td>0.082132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Undersampled</th>\n",
       "      <td>0.888454</td>\n",
       "      <td>0.696340</td>\n",
       "      <td>0.009238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oversampled</th>\n",
       "      <td>0.982024</td>\n",
       "      <td>0.824616</td>\n",
       "      <td>0.078634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Train      Test       PCL\n",
       "Un-sampled    0.921063  0.902687  0.082132\n",
       "Undersampled  0.888454  0.696340  0.009238\n",
       "Oversampled   0.982024  0.824616  0.078634"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_function = Support_Vector_Machine()\n",
    "print(\"Ideal C and gamma parameters\", SVM_function[1])\n",
    "SVM_results = SVM_function[0]\n",
    "SVM_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 5 Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marcu\\AppData\\Local\\Temp\\ipykernel_10168\\1050191320.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  i[num_vars] = scaler.fit_transform(i[num_vars])\n",
      "C:\\Users\\marcu\\AppData\\Local\\Temp\\ipykernel_10168\\1050191320.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  i[num_vars] = scaler.fit_transform(i[num_vars])\n",
      "C:\\Users\\marcu\\AppData\\Local\\Temp\\ipykernel_10168\\1050191320.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  i[num_vars] = scaler.fit_transform(i[num_vars])\n"
     ]
    }
   ],
   "source": [
    "# Scaling the numeric variables\n",
    "# This changes the original datasets\n",
    "num_vars = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "for i in data:\n",
    "  scaler = MinMaxScaler()\n",
    "  i[num_vars] = scaler.fit_transform(i[num_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Artificial_Neural_Network():\n",
    "\n",
    "    train_res = []\n",
    "    test_res = []\n",
    "    pcl_res = []\n",
    "\n",
    "\n",
    "    for i in range(0,3):\n",
    "        model = Sequential()\n",
    "\n",
    "         # Build the neural network\n",
    "        model.add(Dense(32, input_dim = data[i].shape[1], activation=\"leaky_relu\"))\n",
    "        model.add(Dense(16, activation=\"leaky_relu\"))\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(x=data[i], y=targets[i], epochs=20, batch_size=10, verbose=1, validation_split=.1)\n",
    "\n",
    "        # Predictions \n",
    "        train_predictions = model.predict(x=data[i], batch_size=10, verbose=0)\n",
    "        test_predictions = model.predict(x=test_data, batch_size=10, verbose=0)\n",
    "\n",
    "        # Binary predictions\n",
    "        train_pred = [np.round(num) for num in train_predictions]\n",
    "        test_pred = [np.round(num) for num in test_predictions]\n",
    "\n",
    "\n",
    "        cm = confusion_matrix(train_pred, targets[i])\n",
    "        cm = cm/np.sum(cm)\n",
    "        train_res.append(cm[0][0]+cm[1][1])\n",
    "\n",
    "        cm = confusion_matrix(test_pred, test_target)\n",
    "        cm = cm/np.sum(cm)\n",
    "        test_res.append(cm[0][0]+cm[1][1])\n",
    "\n",
    "        cm = cm[0][1]/(cm[0][1]+cm[0][0])\n",
    "        pcl_res.append(cm)\n",
    "\n",
    "    df = pd.DataFrame({\"Train\": train_res, \"Test\": test_res, \"PCL\": pcl_res}, index=[\"Un-sampled\", \"Undersampled\", \"Oversampled\"])\n",
    "\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3256/3256 [==============================] - 14s 3ms/step - loss: 0.2710 - accuracy: 0.8958 - val_loss: 0.2285 - val_accuracy: 0.9041\n",
      "Epoch 2/20\n",
      "3256/3256 [==============================] - 11s 3ms/step - loss: 0.2291 - accuracy: 0.9020 - val_loss: 0.2187 - val_accuracy: 0.9071\n",
      "Epoch 3/20\n",
      "3256/3256 [==============================] - 11s 3ms/step - loss: 0.2208 - accuracy: 0.9036 - val_loss: 0.2139 - val_accuracy: 0.9090\n",
      "Epoch 4/20\n",
      "3256/3256 [==============================] - 11s 3ms/step - loss: 0.2161 - accuracy: 0.9041 - val_loss: 0.2108 - val_accuracy: 0.9110\n",
      "Epoch 5/20\n",
      "3256/3256 [==============================] - 10s 3ms/step - loss: 0.2119 - accuracy: 0.9040 - val_loss: 0.2102 - val_accuracy: 0.9107\n",
      "Epoch 6/20\n",
      "3256/3256 [==============================] - 12s 4ms/step - loss: 0.2084 - accuracy: 0.9053 - val_loss: 0.2053 - val_accuracy: 0.9101\n",
      "Epoch 7/20\n",
      "3256/3256 [==============================] - 13s 4ms/step - loss: 0.2058 - accuracy: 0.9072 - val_loss: 0.2057 - val_accuracy: 0.9129\n",
      "Epoch 8/20\n",
      "3256/3256 [==============================] - 11s 3ms/step - loss: 0.2044 - accuracy: 0.9060 - val_loss: 0.2046 - val_accuracy: 0.9107\n",
      "Epoch 9/20\n",
      "3256/3256 [==============================] - 11s 3ms/step - loss: 0.2026 - accuracy: 0.9081 - val_loss: 0.1987 - val_accuracy: 0.9107\n",
      "Epoch 10/20\n",
      "3256/3256 [==============================] - 11s 4ms/step - loss: 0.2010 - accuracy: 0.9081 - val_loss: 0.1986 - val_accuracy: 0.9088\n",
      "Epoch 11/20\n",
      "3256/3256 [==============================] - 11s 3ms/step - loss: 0.1991 - accuracy: 0.9100 - val_loss: 0.2034 - val_accuracy: 0.9110\n",
      "Epoch 12/20\n",
      "3256/3256 [==============================] - 11s 3ms/step - loss: 0.1976 - accuracy: 0.9098 - val_loss: 0.2022 - val_accuracy: 0.9104\n",
      "Epoch 13/20\n",
      "3256/3256 [==============================] - 11s 3ms/step - loss: 0.1972 - accuracy: 0.9108 - val_loss: 0.1976 - val_accuracy: 0.9124\n",
      "Epoch 14/20\n",
      "3256/3256 [==============================] - 11s 3ms/step - loss: 0.1952 - accuracy: 0.9111 - val_loss: 0.1970 - val_accuracy: 0.9085\n",
      "Epoch 15/20\n",
      "3256/3256 [==============================] - 11s 3ms/step - loss: 0.1942 - accuracy: 0.9108 - val_loss: 0.2035 - val_accuracy: 0.9093\n",
      "Epoch 16/20\n",
      "3256/3256 [==============================] - 11s 3ms/step - loss: 0.1933 - accuracy: 0.9109 - val_loss: 0.2012 - val_accuracy: 0.9068\n",
      "Epoch 17/20\n",
      "3256/3256 [==============================] - 11s 3ms/step - loss: 0.1925 - accuracy: 0.9121 - val_loss: 0.1963 - val_accuracy: 0.9104\n",
      "Epoch 18/20\n",
      "3256/3256 [==============================] - 11s 3ms/step - loss: 0.1919 - accuracy: 0.9127 - val_loss: 0.1964 - val_accuracy: 0.9104\n",
      "Epoch 19/20\n",
      "3256/3256 [==============================] - 11s 3ms/step - loss: 0.1911 - accuracy: 0.9125 - val_loss: 0.2005 - val_accuracy: 0.9063\n",
      "Epoch 20/20\n",
      "3256/3256 [==============================] - 11s 3ms/step - loss: 0.1902 - accuracy: 0.9131 - val_loss: 0.1963 - val_accuracy: 0.9099\n",
      "Epoch 1/20\n",
      "760/760 [==============================] - 5s 4ms/step - loss: 0.5573 - accuracy: 0.7068 - val_loss: 0.5432 - val_accuracy: 0.7500\n",
      "Epoch 2/20\n",
      "760/760 [==============================] - 3s 3ms/step - loss: 0.4043 - accuracy: 0.8346 - val_loss: 0.4539 - val_accuracy: 0.8270\n",
      "Epoch 3/20\n",
      "760/760 [==============================] - 3s 4ms/step - loss: 0.3687 - accuracy: 0.8498 - val_loss: 0.5216 - val_accuracy: 0.7583\n",
      "Epoch 4/20\n",
      "760/760 [==============================] - 3s 4ms/step - loss: 0.3597 - accuracy: 0.8522 - val_loss: 0.4283 - val_accuracy: 0.8116\n",
      "Epoch 5/20\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 0.3515 - accuracy: 0.8559 - val_loss: 0.4921 - val_accuracy: 0.7701\n",
      "Epoch 6/20\n",
      "760/760 [==============================] - 3s 3ms/step - loss: 0.3464 - accuracy: 0.8593 - val_loss: 0.3765 - val_accuracy: 0.8353\n",
      "Epoch 7/20\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 0.3405 - accuracy: 0.8585 - val_loss: 0.5983 - val_accuracy: 0.7156\n",
      "Epoch 8/20\n",
      "760/760 [==============================] - 3s 3ms/step - loss: 0.3356 - accuracy: 0.8655 - val_loss: 0.4907 - val_accuracy: 0.7690\n",
      "Epoch 9/20\n",
      "760/760 [==============================] - 3s 4ms/step - loss: 0.3315 - accuracy: 0.8649 - val_loss: 0.4317 - val_accuracy: 0.8057\n",
      "Epoch 10/20\n",
      "760/760 [==============================] - 3s 4ms/step - loss: 0.3279 - accuracy: 0.8689 - val_loss: 0.4856 - val_accuracy: 0.7820\n",
      "Epoch 11/20\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 0.3237 - accuracy: 0.8712 - val_loss: 0.4660 - val_accuracy: 0.7820\n",
      "Epoch 12/20\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 0.3194 - accuracy: 0.8717 - val_loss: 0.3475 - val_accuracy: 0.8353\n",
      "Epoch 13/20\n",
      "760/760 [==============================] - 3s 4ms/step - loss: 0.3171 - accuracy: 0.8707 - val_loss: 0.4122 - val_accuracy: 0.8104\n",
      "Epoch 14/20\n",
      "760/760 [==============================] - 3s 4ms/step - loss: 0.3125 - accuracy: 0.8724 - val_loss: 0.4830 - val_accuracy: 0.7784\n",
      "Epoch 15/20\n",
      "760/760 [==============================] - 3s 3ms/step - loss: 0.3130 - accuracy: 0.8753 - val_loss: 0.3773 - val_accuracy: 0.8175\n",
      "Epoch 16/20\n",
      "760/760 [==============================] - 3s 3ms/step - loss: 0.3095 - accuracy: 0.8764 - val_loss: 0.5856 - val_accuracy: 0.7299\n",
      "Epoch 17/20\n",
      "760/760 [==============================] - 3s 4ms/step - loss: 0.3060 - accuracy: 0.8761 - val_loss: 0.4234 - val_accuracy: 0.7950\n",
      "Epoch 18/20\n",
      "760/760 [==============================] - 3s 3ms/step - loss: 0.3035 - accuracy: 0.8788 - val_loss: 0.4334 - val_accuracy: 0.7927\n",
      "Epoch 19/20\n",
      "760/760 [==============================] - 3s 3ms/step - loss: 0.3018 - accuracy: 0.8800 - val_loss: 0.3877 - val_accuracy: 0.8045\n",
      "Epoch 20/20\n",
      "760/760 [==============================] - 2s 3ms/step - loss: 0.3023 - accuracy: 0.8772 - val_loss: 0.3723 - val_accuracy: 0.8152\n",
      "Epoch 1/20\n",
      "5728/5728 [==============================] - 21s 3ms/step - loss: 0.3981 - accuracy: 0.8258 - val_loss: 0.4997 - val_accuracy: 0.7684\n",
      "Epoch 2/20\n",
      "5728/5728 [==============================] - 21s 4ms/step - loss: 0.3357 - accuracy: 0.8621 - val_loss: 0.4704 - val_accuracy: 0.7733\n",
      "Epoch 3/20\n",
      "5728/5728 [==============================] - 21s 4ms/step - loss: 0.3187 - accuracy: 0.8695 - val_loss: 0.3682 - val_accuracy: 0.8222\n",
      "Epoch 4/20\n",
      "5728/5728 [==============================] - 19s 3ms/step - loss: 0.3086 - accuracy: 0.8738 - val_loss: 0.3738 - val_accuracy: 0.8207\n",
      "Epoch 5/20\n",
      "5728/5728 [==============================] - 18s 3ms/step - loss: 0.3015 - accuracy: 0.8795 - val_loss: 0.3137 - val_accuracy: 0.8454\n",
      "Epoch 6/20\n",
      "5728/5728 [==============================] - 19s 3ms/step - loss: 0.2959 - accuracy: 0.8820 - val_loss: 0.4231 - val_accuracy: 0.8046\n",
      "Epoch 7/20\n",
      "5728/5728 [==============================] - 19s 3ms/step - loss: 0.2923 - accuracy: 0.8829 - val_loss: 0.3888 - val_accuracy: 0.8119\n",
      "Epoch 8/20\n",
      "5728/5728 [==============================] - 22s 4ms/step - loss: 0.2892 - accuracy: 0.8840 - val_loss: 0.4463 - val_accuracy: 0.7855\n",
      "Epoch 9/20\n",
      "5728/5728 [==============================] - 17s 3ms/step - loss: 0.2861 - accuracy: 0.8856 - val_loss: 0.3381 - val_accuracy: 0.8357\n",
      "Epoch 10/20\n",
      "5728/5728 [==============================] - 18s 3ms/step - loss: 0.2835 - accuracy: 0.8862 - val_loss: 0.3627 - val_accuracy: 0.8182\n",
      "Epoch 11/20\n",
      "5728/5728 [==============================] - 18s 3ms/step - loss: 0.2814 - accuracy: 0.8872 - val_loss: 0.3743 - val_accuracy: 0.8190\n",
      "Epoch 12/20\n",
      "5728/5728 [==============================] - 18s 3ms/step - loss: 0.2785 - accuracy: 0.8885 - val_loss: 0.4346 - val_accuracy: 0.7855\n",
      "Epoch 13/20\n",
      "5728/5728 [==============================] - 18s 3ms/step - loss: 0.2774 - accuracy: 0.8903 - val_loss: 0.4062 - val_accuracy: 0.8017\n",
      "Epoch 14/20\n",
      "5728/5728 [==============================] - 19s 3ms/step - loss: 0.2756 - accuracy: 0.8915 - val_loss: 0.4080 - val_accuracy: 0.8039\n",
      "Epoch 15/20\n",
      "5728/5728 [==============================] - 19s 3ms/step - loss: 0.2740 - accuracy: 0.8915 - val_loss: 0.4279 - val_accuracy: 0.8030\n",
      "Epoch 16/20\n",
      "5728/5728 [==============================] - 18s 3ms/step - loss: 0.2726 - accuracy: 0.8922 - val_loss: 0.3763 - val_accuracy: 0.8184\n",
      "Epoch 17/20\n",
      "5728/5728 [==============================] - 18s 3ms/step - loss: 0.2714 - accuracy: 0.8931 - val_loss: 0.3446 - val_accuracy: 0.8313\n",
      "Epoch 18/20\n",
      "5728/5728 [==============================] - 18s 3ms/step - loss: 0.2697 - accuracy: 0.8939 - val_loss: 0.4720 - val_accuracy: 0.7859\n",
      "Epoch 19/20\n",
      "5728/5728 [==============================] - 19s 3ms/step - loss: 0.2684 - accuracy: 0.8944 - val_loss: 0.3350 - val_accuracy: 0.8452\n",
      "Epoch 20/20\n",
      "5728/5728 [==============================] - 18s 3ms/step - loss: 0.2669 - accuracy: 0.8960 - val_loss: 0.3779 - val_accuracy: 0.8248\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>PCL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Un-sampled</th>\n",
       "      <td>0.915865</td>\n",
       "      <td>0.301006</td>\n",
       "      <td>0.08072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Undersampled</th>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.125069</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oversampled</th>\n",
       "      <td>0.891595</td>\n",
       "      <td>0.120978</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Train      Test      PCL\n",
       "Un-sampled    0.915865  0.301006  0.08072\n",
       "Undersampled  0.877193  0.125069  0.00000\n",
       "Oversampled   0.891595  0.120978  0.00000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANN_results = Artificial_Neural_Network()\n",
    "ANN_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
